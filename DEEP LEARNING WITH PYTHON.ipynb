{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need some categorical points to work through neural networks\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just now have to cast to y data\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of neural networks, it is always a good idea to normaize your data\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_obj = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_obj.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_Xtrain = scaler_obj.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_obj.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_Xtest = scaler_obj.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8,input_dim = 4, activation='relu'))\n",
    "model.add(Dense(8,input_dim = 4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 139\n",
      "Trainable params: 139\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      " - 1s - loss: 1.1427 - acc: 0.0600\n",
      "Epoch 2/150\n",
      " - 0s - loss: 1.1369 - acc: 0.1500\n",
      "Epoch 3/150\n",
      " - 0s - loss: 1.1318 - acc: 0.2800\n",
      "Epoch 4/150\n",
      " - 0s - loss: 1.1272 - acc: 0.3500\n",
      "Epoch 5/150\n",
      " - 0s - loss: 1.1230 - acc: 0.3500\n",
      "Epoch 6/150\n",
      " - 0s - loss: 1.1189 - acc: 0.3500\n",
      "Epoch 7/150\n",
      " - 0s - loss: 1.1147 - acc: 0.3500\n",
      "Epoch 8/150\n",
      " - 0s - loss: 1.1104 - acc: 0.3500\n",
      "Epoch 9/150\n",
      " - 0s - loss: 1.1063 - acc: 0.3500\n",
      "Epoch 10/150\n",
      " - 0s - loss: 1.1022 - acc: 0.3500\n",
      "Epoch 11/150\n",
      " - 0s - loss: 1.0981 - acc: 0.3500\n",
      "Epoch 12/150\n",
      " - 0s - loss: 1.0943 - acc: 0.3500\n",
      "Epoch 13/150\n",
      " - 0s - loss: 1.0907 - acc: 0.3500\n",
      "Epoch 14/150\n",
      " - 0s - loss: 1.0873 - acc: 0.3500\n",
      "Epoch 15/150\n",
      " - 0s - loss: 1.0838 - acc: 0.3500\n",
      "Epoch 16/150\n",
      " - 0s - loss: 1.0803 - acc: 0.3500\n",
      "Epoch 17/150\n",
      " - 0s - loss: 1.0767 - acc: 0.3500\n",
      "Epoch 18/150\n",
      " - 0s - loss: 1.0733 - acc: 0.3500\n",
      "Epoch 19/150\n",
      " - 0s - loss: 1.0696 - acc: 0.3500\n",
      "Epoch 20/150\n",
      " - 0s - loss: 1.0661 - acc: 0.3500\n",
      "Epoch 21/150\n",
      " - 0s - loss: 1.0622 - acc: 0.3500\n",
      "Epoch 22/150\n",
      " - 0s - loss: 1.0585 - acc: 0.3500\n",
      "Epoch 23/150\n",
      " - 0s - loss: 1.0546 - acc: 0.4000\n",
      "Epoch 24/150\n",
      " - 0s - loss: 1.0508 - acc: 0.4700\n",
      "Epoch 25/150\n",
      " - 0s - loss: 1.0467 - acc: 0.5900\n",
      "Epoch 26/150\n",
      " - 0s - loss: 1.0424 - acc: 0.6300\n",
      "Epoch 27/150\n",
      " - 0s - loss: 1.0380 - acc: 0.6500\n",
      "Epoch 28/150\n",
      " - 0s - loss: 1.0333 - acc: 0.6500\n",
      "Epoch 29/150\n",
      " - 0s - loss: 1.0281 - acc: 0.6500\n",
      "Epoch 30/150\n",
      " - 0s - loss: 1.0228 - acc: 0.6500\n",
      "Epoch 31/150\n",
      " - 0s - loss: 1.0174 - acc: 0.6500\n",
      "Epoch 32/150\n",
      " - 0s - loss: 1.0119 - acc: 0.6500\n",
      "Epoch 33/150\n",
      " - 0s - loss: 1.0062 - acc: 0.6500\n",
      "Epoch 34/150\n",
      " - 0s - loss: 1.0003 - acc: 0.6500\n",
      "Epoch 35/150\n",
      " - 0s - loss: 0.9944 - acc: 0.6500\n",
      "Epoch 36/150\n",
      " - 0s - loss: 0.9881 - acc: 0.6500\n",
      "Epoch 37/150\n",
      " - 0s - loss: 0.9814 - acc: 0.6500\n",
      "Epoch 38/150\n",
      " - 0s - loss: 0.9745 - acc: 0.6500\n",
      "Epoch 39/150\n",
      " - 0s - loss: 0.9678 - acc: 0.6500\n",
      "Epoch 40/150\n",
      " - 0s - loss: 0.9604 - acc: 0.6500\n",
      "Epoch 41/150\n",
      " - 0s - loss: 0.9529 - acc: 0.6500\n",
      "Epoch 42/150\n",
      " - 0s - loss: 0.9452 - acc: 0.6500\n",
      "Epoch 43/150\n",
      " - 0s - loss: 0.9374 - acc: 0.6500\n",
      "Epoch 44/150\n",
      " - 0s - loss: 0.9294 - acc: 0.6500\n",
      "Epoch 45/150\n",
      " - 0s - loss: 0.9205 - acc: 0.6500\n",
      "Epoch 46/150\n",
      " - 0s - loss: 0.9125 - acc: 0.6500\n",
      "Epoch 47/150\n",
      " - 0s - loss: 0.9030 - acc: 0.6600\n",
      "Epoch 48/150\n",
      " - 0s - loss: 0.8931 - acc: 0.6600\n",
      "Epoch 49/150\n",
      " - 0s - loss: 0.8837 - acc: 0.6600\n",
      "Epoch 50/150\n",
      " - 0s - loss: 0.8741 - acc: 0.6600\n",
      "Epoch 51/150\n",
      " - 0s - loss: 0.8642 - acc: 0.6600\n",
      "Epoch 52/150\n",
      " - 0s - loss: 0.8542 - acc: 0.6600\n",
      "Epoch 53/150\n",
      " - 0s - loss: 0.8443 - acc: 0.6600\n",
      "Epoch 54/150\n",
      " - 0s - loss: 0.8337 - acc: 0.6600\n",
      "Epoch 55/150\n",
      " - 0s - loss: 0.8231 - acc: 0.6600\n",
      "Epoch 56/150\n",
      " - 0s - loss: 0.8122 - acc: 0.6600\n",
      "Epoch 57/150\n",
      " - 0s - loss: 0.8018 - acc: 0.6600\n",
      "Epoch 58/150\n",
      " - 0s - loss: 0.7906 - acc: 0.6600\n",
      "Epoch 59/150\n",
      " - 0s - loss: 0.7803 - acc: 0.6600\n",
      "Epoch 60/150\n",
      " - 0s - loss: 0.7700 - acc: 0.6600\n",
      "Epoch 61/150\n",
      " - 0s - loss: 0.7592 - acc: 0.6600\n",
      "Epoch 62/150\n",
      " - 0s - loss: 0.7487 - acc: 0.6600\n",
      "Epoch 63/150\n",
      " - 0s - loss: 0.7392 - acc: 0.6600\n",
      "Epoch 64/150\n",
      " - 0s - loss: 0.7294 - acc: 0.6600\n",
      "Epoch 65/150\n",
      " - 0s - loss: 0.7196 - acc: 0.6600\n",
      "Epoch 66/150\n",
      " - 0s - loss: 0.7100 - acc: 0.6600\n",
      "Epoch 67/150\n",
      " - 0s - loss: 0.7008 - acc: 0.6600\n",
      "Epoch 68/150\n",
      " - 0s - loss: 0.6914 - acc: 0.6600\n",
      "Epoch 69/150\n",
      " - 0s - loss: 0.6821 - acc: 0.6600\n",
      "Epoch 70/150\n",
      " - 0s - loss: 0.6732 - acc: 0.6600\n",
      "Epoch 71/150\n",
      " - 0s - loss: 0.6643 - acc: 0.6600\n",
      "Epoch 72/150\n",
      " - 0s - loss: 0.6558 - acc: 0.6600\n",
      "Epoch 73/150\n",
      " - 0s - loss: 0.6479 - acc: 0.6600\n",
      "Epoch 74/150\n",
      " - 0s - loss: 0.6400 - acc: 0.6600\n",
      "Epoch 75/150\n",
      " - 0s - loss: 0.6326 - acc: 0.6600\n",
      "Epoch 76/150\n",
      " - 0s - loss: 0.6252 - acc: 0.6600\n",
      "Epoch 77/150\n",
      " - 0s - loss: 0.6182 - acc: 0.6600\n",
      "Epoch 78/150\n",
      " - 0s - loss: 0.6116 - acc: 0.6600\n",
      "Epoch 79/150\n",
      " - 0s - loss: 0.6055 - acc: 0.6600\n",
      "Epoch 80/150\n",
      " - 0s - loss: 0.5996 - acc: 0.6600\n",
      "Epoch 81/150\n",
      " - 0s - loss: 0.5936 - acc: 0.6600\n",
      "Epoch 82/150\n",
      " - 0s - loss: 0.5881 - acc: 0.6600\n",
      "Epoch 83/150\n",
      " - 0s - loss: 0.5829 - acc: 0.6600\n",
      "Epoch 84/150\n",
      " - 0s - loss: 0.5780 - acc: 0.6600\n",
      "Epoch 85/150\n",
      " - 0s - loss: 0.5735 - acc: 0.6600\n",
      "Epoch 86/150\n",
      " - 0s - loss: 0.5687 - acc: 0.6600\n",
      "Epoch 87/150\n",
      " - 0s - loss: 0.5638 - acc: 0.6600\n",
      "Epoch 88/150\n",
      " - 0s - loss: 0.5596 - acc: 0.6600\n",
      "Epoch 89/150\n",
      " - 0s - loss: 0.5553 - acc: 0.6600\n",
      "Epoch 90/150\n",
      " - 0s - loss: 0.5502 - acc: 0.6600\n",
      "Epoch 91/150\n",
      " - 0s - loss: 0.5461 - acc: 0.6600\n",
      "Epoch 92/150\n",
      " - 0s - loss: 0.5422 - acc: 0.6600\n",
      "Epoch 93/150\n",
      " - 0s - loss: 0.5382 - acc: 0.6600\n",
      "Epoch 94/150\n",
      " - 0s - loss: 0.5342 - acc: 0.6600\n",
      "Epoch 95/150\n",
      " - 0s - loss: 0.5313 - acc: 0.6600\n",
      "Epoch 96/150\n",
      " - 0s - loss: 0.5270 - acc: 0.6600\n",
      "Epoch 97/150\n",
      " - 0s - loss: 0.5238 - acc: 0.6600\n",
      "Epoch 98/150\n",
      " - 0s - loss: 0.5201 - acc: 0.6600\n",
      "Epoch 99/150\n",
      " - 0s - loss: 0.5176 - acc: 0.6600\n",
      "Epoch 100/150\n",
      " - 0s - loss: 0.5145 - acc: 0.6600\n",
      "Epoch 101/150\n",
      " - 0s - loss: 0.5116 - acc: 0.6600\n",
      "Epoch 102/150\n",
      " - 0s - loss: 0.5087 - acc: 0.6600\n",
      "Epoch 103/150\n",
      " - 0s - loss: 0.5058 - acc: 0.6600\n",
      "Epoch 104/150\n",
      " - 0s - loss: 0.5029 - acc: 0.6600\n",
      "Epoch 105/150\n",
      " - 0s - loss: 0.5004 - acc: 0.6600\n",
      "Epoch 106/150\n",
      " - 0s - loss: 0.4979 - acc: 0.6600\n",
      "Epoch 107/150\n",
      " - 0s - loss: 0.4955 - acc: 0.6600\n",
      "Epoch 108/150\n",
      " - 0s - loss: 0.4933 - acc: 0.6600\n",
      "Epoch 109/150\n",
      " - 0s - loss: 0.4910 - acc: 0.6600\n",
      "Epoch 110/150\n",
      " - 0s - loss: 0.4888 - acc: 0.6600\n",
      "Epoch 111/150\n",
      " - 0s - loss: 0.4868 - acc: 0.6600\n",
      "Epoch 112/150\n",
      " - 0s - loss: 0.4851 - acc: 0.6600\n",
      "Epoch 113/150\n",
      " - 0s - loss: 0.4829 - acc: 0.6600\n",
      "Epoch 114/150\n",
      " - 0s - loss: 0.4810 - acc: 0.6700\n",
      "Epoch 115/150\n",
      " - 0s - loss: 0.4791 - acc: 0.6700\n",
      "Epoch 116/150\n",
      " - 0s - loss: 0.4772 - acc: 0.6800\n",
      "Epoch 117/150\n",
      " - 0s - loss: 0.4754 - acc: 0.6800\n",
      "Epoch 118/150\n",
      " - 0s - loss: 0.4737 - acc: 0.6800\n",
      "Epoch 119/150\n",
      " - 0s - loss: 0.4720 - acc: 0.6800\n",
      "Epoch 120/150\n",
      " - 0s - loss: 0.4705 - acc: 0.6700\n",
      "Epoch 121/150\n",
      " - 0s - loss: 0.4685 - acc: 0.6800\n",
      "Epoch 122/150\n",
      " - 0s - loss: 0.4667 - acc: 0.6800\n",
      "Epoch 123/150\n",
      " - 0s - loss: 0.4652 - acc: 0.6800\n",
      "Epoch 124/150\n",
      " - 0s - loss: 0.4643 - acc: 0.6900\n",
      "Epoch 125/150\n",
      " - 0s - loss: 0.4619 - acc: 0.6900\n",
      "Epoch 126/150\n",
      " - 0s - loss: 0.4608 - acc: 0.7000\n",
      "Epoch 127/150\n",
      " - 0s - loss: 0.4596 - acc: 0.7100\n",
      "Epoch 128/150\n",
      " - 0s - loss: 0.4580 - acc: 0.7100\n",
      "Epoch 129/150\n",
      " - 0s - loss: 0.4566 - acc: 0.7100\n",
      "Epoch 130/150\n",
      " - 0s - loss: 0.4552 - acc: 0.7100\n",
      "Epoch 131/150\n",
      " - 0s - loss: 0.4542 - acc: 0.7500\n",
      "Epoch 132/150\n",
      " - 0s - loss: 0.4533 - acc: 0.7800\n",
      "Epoch 133/150\n",
      " - 0s - loss: 0.4526 - acc: 0.8100\n",
      "Epoch 134/150\n",
      " - 0s - loss: 0.4514 - acc: 0.8300\n",
      "Epoch 135/150\n",
      " - 0s - loss: 0.4497 - acc: 0.8200\n",
      "Epoch 136/150\n",
      " - 0s - loss: 0.4480 - acc: 0.7900\n",
      "Epoch 137/150\n",
      " - 0s - loss: 0.4461 - acc: 0.7500\n",
      "Epoch 138/150\n",
      " - 0s - loss: 0.4448 - acc: 0.7200\n",
      "Epoch 139/150\n",
      " - 0s - loss: 0.4440 - acc: 0.7100\n",
      "Epoch 140/150\n",
      " - 0s - loss: 0.4423 - acc: 0.7100\n",
      "Epoch 141/150\n",
      " - 0s - loss: 0.4411 - acc: 0.7200\n",
      "Epoch 142/150\n",
      " - 0s - loss: 0.4400 - acc: 0.7400\n",
      "Epoch 143/150\n",
      " - 0s - loss: 0.4391 - acc: 0.7800\n",
      "Epoch 144/150\n",
      " - 0s - loss: 0.4385 - acc: 0.8200\n",
      "Epoch 145/150\n",
      " - 0s - loss: 0.4377 - acc: 0.8500\n",
      "Epoch 146/150\n",
      " - 0s - loss: 0.4366 - acc: 0.8500\n",
      "Epoch 147/150\n",
      " - 0s - loss: 0.4352 - acc: 0.8300\n",
      "Epoch 148/150\n",
      " - 0s - loss: 0.4338 - acc: 0.8200\n",
      "Epoch 149/150\n",
      " - 0s - loss: 0.4322 - acc: 0.8200\n",
      "Epoch 150/150\n",
      " - 0s - loss: 0.4307 - acc: 0.7800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17390aa9488>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_Xtrain,y_train, epochs=150,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(scaled_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 2, 2, 1, 1, 2, 0, 1, 2, 2, 0, 2, 1, 0, 1, 2, 0, 1, 2,\n",
       "       2, 1, 1, 2, 2, 0, 0, 0, 2, 2, 1, 2, 0, 1, 1, 2, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19,  0,  0],\n",
       "       [ 0,  2, 14],\n",
       "       [ 0,  0, 15]], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test.argmax(axis=1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.12      0.22        16\n",
      "           2       0.52      1.00      0.68        15\n",
      "\n",
      "   micro avg       0.72      0.72      0.72        50\n",
      "   macro avg       0.84      0.71      0.63        50\n",
      "weighted avg       0.86      0.72      0.66        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test.argmax(axis=1), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Keras and LSTM using GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convinient function to read the files\n",
    "\n",
    "def file_access(filename):\n",
    "    with open(filename) as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_access('../UPDATED_NLP_COURSE/06-Deep-Learning/moby_dick_four_chapters.txt') # lot of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy to clean and tokenize text\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(file_access('../UPDATED_NLP_COURSE/06-Deep-Learning/moby_dick_four_chapters.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(doc_text):\n",
    "    return [token.text.lower() for token in doc_text if token.text not in '\\n\\n  \\n\\n\\n!\"#$%&\\'()*+,-- -./:;<=>?@[\\\\]^_`{|}~ \\t \\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text_cleaner(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will create sequences of text from the document which will basically predict the 26th word from the previous 25 words\n",
    "\n",
    "train_len = 25 + 1\n",
    "\n",
    "text_seq = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    text_seq.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to convert these sequences to numerical tokens for keras to understand\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tokenizer object\n",
    "tokenizer =Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2707"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whenever you want to check the mapping of the tokens to sequences\n",
    "len(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2707"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many times each word appears in the text\n",
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since it is better to view it as a numpy array, we can cast it into a nice array\n",
    "\n",
    "import numpy as np\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 962,   14,  264, ..., 2702,   14,   24],\n",
       "       [  14,  264,   51, ...,   14,   24,  963],\n",
       "       [ 264,   51,  262, ...,   24,  963,    5],\n",
       "       ...,\n",
       "       [ 958,   12,  167, ...,  263,   53,    2],\n",
       "       [  12,  167, 2701, ...,   53,    2, 2707],\n",
       "       [ 167, 2701,    3, ...,    2, 2707,   26]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking all the values except the last one as features values\n",
    "X = sequences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the last value as the y_train\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes =vocab_size +1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11351, 25)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has been set as 25 for future adjustments\n",
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, seq_len):\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, seq_len, input_length=seq_len))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 25, 25)            67700     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 25, 50)            15200     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2708)              138108    \n",
      "=================================================================\n",
      "Total params: 243,758\n",
      "Trainable params: 243,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "11351/11351 [==============================] - 23s 2ms/step - loss: 4.5984 - acc: 0.1162\n",
      "Epoch 2/300\n",
      "11351/11351 [==============================] - 10s 862us/step - loss: 4.5704 - acc: 0.1179\n",
      "Epoch 3/300\n",
      "11351/11351 [==============================] - 10s 911us/step - loss: 4.5420 - acc: 0.12092s - loss: 4.5\n",
      "Epoch 4/300\n",
      "11351/11351 [==============================] - 10s 907us/step - loss: 4.5139 - acc: 0.1189\n",
      "Epoch 5/300\n",
      "11351/11351 [==============================] - 10s 885us/step - loss: 4.4858 - acc: 0.1233\n",
      "Epoch 6/300\n",
      "11351/11351 [==============================] - 11s 981us/step - loss: 4.4597 - acc: 0.1255\n",
      "Epoch 7/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 4.4293 - acc: 0.1262\n",
      "Epoch 8/300\n",
      "11351/11351 [==============================] - 11s 964us/step - loss: 4.3958 - acc: 0.1297\n",
      "Epoch 9/300\n",
      "11351/11351 [==============================] - 9s 831us/step - loss: 4.3712 - acc: 0.1337\n",
      "Epoch 10/300\n",
      "11351/11351 [==============================] - 10s 875us/step - loss: 4.3381 - acc: 0.1329\n",
      "Epoch 11/300\n",
      "11351/11351 [==============================] - 9s 825us/step - loss: 4.3123 - acc: 0.1349\n",
      "Epoch 12/300\n",
      "11351/11351 [==============================] - 10s 903us/step - loss: 4.2868 - acc: 0.1358\n",
      "Epoch 13/300\n",
      "11351/11351 [==============================] - 10s 841us/step - loss: 4.2618 - acc: 0.1384\n",
      "Epoch 14/300\n",
      "11351/11351 [==============================] - 11s 926us/step - loss: 4.2319 - acc: 0.1405\n",
      "Epoch 15/300\n",
      "11351/11351 [==============================] - 10s 875us/step - loss: 4.2042 - acc: 0.1438\n",
      "Epoch 16/300\n",
      "11351/11351 [==============================] - 11s 940us/step - loss: 4.1763 - acc: 0.1437\n",
      "Epoch 17/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 4.1532 - acc: 0.1460\n",
      "Epoch 18/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 4.1274 - acc: 0.1462\n",
      "Epoch 19/300\n",
      "11351/11351 [==============================] - 14s 1ms/step - loss: 4.1018 - acc: 0.1525\n",
      "Epoch 20/300\n",
      "11351/11351 [==============================] - 17s 1ms/step - loss: 4.0733 - acc: 0.1568\n",
      "Epoch 21/300\n",
      "11351/11351 [==============================] - 11s 948us/step - loss: 4.0472 - acc: 0.1552\n",
      "Epoch 22/300\n",
      "11351/11351 [==============================] - 11s 1ms/step - loss: 4.0234 - acc: 0.1552\n",
      "Epoch 23/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 4.0008 - acc: 0.1553\n",
      "Epoch 24/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 3.9754 - acc: 0.1603: 5s - loss:\n",
      "Epoch 25/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 3.9556 - acc: 0.1627\n",
      "Epoch 26/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 3.9316 - acc: 0.1625\n",
      "Epoch 27/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 3.9032 - acc: 0.1687\n",
      "Epoch 28/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 3.8847 - acc: 0.1671\n",
      "Epoch 29/300\n",
      "11351/11351 [==============================] - 11s 954us/step - loss: 3.8647 - acc: 0.1721\n",
      "Epoch 30/300\n",
      "11351/11351 [==============================] - 10s 859us/step - loss: 3.8494 - acc: 0.1722\n",
      "Epoch 31/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 3.8253 - acc: 0.1739\n",
      "Epoch 32/300\n",
      "11351/11351 [==============================] - 10s 845us/step - loss: 3.8036 - acc: 0.1802\n",
      "Epoch 33/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 3.7855 - acc: 0.1780\n",
      "Epoch 34/300\n",
      "11351/11351 [==============================] - 11s 1ms/step - loss: 3.7745 - acc: 0.1785\n",
      "Epoch 35/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 3.7394 - acc: 0.1849\n",
      "Epoch 36/300\n",
      "11351/11351 [==============================] - 10s 920us/step - loss: 3.7191 - acc: 0.1876\n",
      "Epoch 37/300\n",
      "11351/11351 [==============================] - 11s 952us/step - loss: 3.6994 - acc: 0.1880\n",
      "Epoch 38/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.6856 - acc: 0.1893\n",
      "Epoch 39/300\n",
      "11351/11351 [==============================] - 10s 920us/step - loss: 3.6639 - acc: 0.1941\n",
      "Epoch 40/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.6454 - acc: 0.1962\n",
      "Epoch 41/300\n",
      "11351/11351 [==============================] - 14s 1ms/step - loss: 3.6307 - acc: 0.1974\n",
      "Epoch 42/300\n",
      "11351/11351 [==============================] - 11s 965us/step - loss: 3.6080 - acc: 0.1995\n",
      "Epoch 43/300\n",
      "11351/11351 [==============================] - 11s 938us/step - loss: 3.5890 - acc: 0.2017\n",
      "Epoch 44/300\n",
      "11351/11351 [==============================] - 11s 932us/step - loss: 3.5748 - acc: 0.2004\n",
      "Epoch 45/300\n",
      "11351/11351 [==============================] - 10s 925us/step - loss: 3.5567 - acc: 0.2091\n",
      "Epoch 46/300\n",
      "11351/11351 [==============================] - 10s 910us/step - loss: 3.5408 - acc: 0.2082\n",
      "Epoch 47/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.5232 - acc: 0.2118\n",
      "Epoch 48/300\n",
      "11351/11351 [==============================] - 10s 875us/step - loss: 3.5135 - acc: 0.2140\n",
      "Epoch 49/300\n",
      "11351/11351 [==============================] - 10s 904us/step - loss: 3.4852 - acc: 0.2154\n",
      "Epoch 50/300\n",
      "11351/11351 [==============================] - 10s 887us/step - loss: 3.4707 - acc: 0.2200\n",
      "Epoch 51/300\n",
      "11351/11351 [==============================] - 10s 892us/step - loss: 3.4546 - acc: 0.2225\n",
      "Epoch 52/300\n",
      "11351/11351 [==============================] - 11s 986us/step - loss: 3.4354 - acc: 0.2224\n",
      "Epoch 53/300\n",
      "11351/11351 [==============================] - 10s 889us/step - loss: 3.4131 - acc: 0.2269\n",
      "Epoch 54/300\n",
      "11351/11351 [==============================] - 10s 914us/step - loss: 3.3998 - acc: 0.2307\n",
      "Epoch 55/300\n",
      "11351/11351 [==============================] - 10s 904us/step - loss: 3.3884 - acc: 0.2298\n",
      "Epoch 56/300\n",
      "11351/11351 [==============================] - 11s 930us/step - loss: 3.3728 - acc: 0.2351\n",
      "Epoch 57/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.3568 - acc: 0.2334\n",
      "Epoch 58/300\n",
      "11351/11351 [==============================] - 11s 984us/step - loss: 3.3395 - acc: 0.2369\n",
      "Epoch 59/300\n",
      "11351/11351 [==============================] - 11s 967us/step - loss: 3.3272 - acc: 0.2414\n",
      "Epoch 60/300\n",
      "11351/11351 [==============================] - 11s 976us/step - loss: 3.3221 - acc: 0.2402\n",
      "Epoch 61/300\n",
      "11351/11351 [==============================] - 11s 958us/step - loss: 3.2912 - acc: 0.2478\n",
      "Epoch 62/300\n",
      "11351/11351 [==============================] - 11s 982us/step - loss: 3.2753 - acc: 0.2478\n",
      "Epoch 63/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.2604 - acc: 0.2528\n",
      "Epoch 64/300\n",
      "11351/11351 [==============================] - 11s 964us/step - loss: 3.2439 - acc: 0.2576\n",
      "Epoch 65/300\n",
      "11351/11351 [==============================] - 11s 984us/step - loss: 3.2318 - acc: 0.2555\n",
      "Epoch 66/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.2174 - acc: 0.2553\n",
      "Epoch 67/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.1995 - acc: 0.2610\n",
      "Epoch 68/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 3.1859 - acc: 0.2637\n",
      "Epoch 69/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.1763 - acc: 0.2638\n",
      "Epoch 70/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.1623 - acc: 0.2700\n",
      "Epoch 71/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 3.1462 - acc: 0.2708\n",
      "Epoch 72/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 3.1288 - acc: 0.2710\n",
      "Epoch 73/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 3.1151 - acc: 0.2782\n",
      "Epoch 74/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.1058 - acc: 0.2805\n",
      "Epoch 75/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.0900 - acc: 0.2844\n",
      "Epoch 76/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.0687 - acc: 0.2853\n",
      "Epoch 77/300\n",
      "11351/11351 [==============================] - 12s 1ms/step - loss: 3.0560 - acc: 0.2877\n",
      "Epoch 78/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 3.0360 - acc: 0.2905\n",
      "Epoch 79/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 3.0244 - acc: 0.2927\n",
      "Epoch 80/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 3.0145 - acc: 0.2960\n",
      "Epoch 81/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 3.0033 - acc: 0.2977\n",
      "Epoch 82/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 3.0031 - acc: 0.2967\n",
      "Epoch 83/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 2.9788 - acc: 0.3015\n",
      "Epoch 84/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 2.9692 - acc: 0.3011\n",
      "Epoch 85/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 2.9496 - acc: 0.3093\n",
      "Epoch 86/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 2.9478 - acc: 0.3070: \n",
      "Epoch 87/300\n",
      "11351/11351 [==============================] - 14s 1ms/step - loss: 2.9465 - acc: 0.3058\n",
      "Epoch 88/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 2.9232 - acc: 0.3108: 7s - loss: 2.8261 - acc:\n",
      "Epoch 89/300\n",
      "11351/11351 [==============================] - 13s 1ms/step - loss: 2.9039 - acc: 0.3180\n",
      "Epoch 90/300\n",
      "11351/11351 [==============================] - 14s 1ms/step - loss: 2.8921 - acc: 0.3194\n",
      "Epoch 91/300\n",
      "11351/11351 [==============================] - 15s 1ms/step - loss: 2.8881 - acc: 0.3193: 5s\n",
      "Epoch 92/300\n",
      "11351/11351 [==============================] - 14s 1ms/step - loss: 2.8674 - acc: 0.3233\n",
      "Epoch 93/300\n",
      "11351/11351 [==============================] - 14s 1ms/step - loss: 2.8629 - acc: 0.3247\n",
      "Epoch 94/300\n",
      "11351/11351 [==============================] - 15s 1ms/step - loss: 2.8475 - acc: 0.3253\n",
      "Epoch 95/300\n",
      "11351/11351 [==============================] - 15s 1ms/step - loss: 2.8362 - acc: 0.3282\n",
      "Epoch 96/300\n",
      "11351/11351 [==============================] - 15s 1ms/step - loss: 2.8208 - acc: 0.3335\n",
      "Epoch 97/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.8135 - acc: 0.3332\n",
      "Epoch 98/300\n",
      "11351/11351 [==============================] - 17s 1ms/step - loss: 2.7968 - acc: 0.3362\n",
      "Epoch 99/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.7811 - acc: 0.3387\n",
      "Epoch 100/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.7713 - acc: 0.3413\n",
      "Epoch 101/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.7606 - acc: 0.3434\n",
      "Epoch 102/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.7605 - acc: 0.3433\n",
      "Epoch 103/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.7417 - acc: 0.3471\n",
      "Epoch 104/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.7334 - acc: 0.3460\n",
      "Epoch 105/300\n",
      "11351/11351 [==============================] - 17s 2ms/step - loss: 2.7174 - acc: 0.3483\n",
      "Epoch 106/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.7113 - acc: 0.3558\n",
      "Epoch 107/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.6994 - acc: 0.3555\n",
      "Epoch 108/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.6934 - acc: 0.3555\n",
      "Epoch 109/300\n",
      "11351/11351 [==============================] - 17s 1ms/step - loss: 2.6692 - acc: 0.3645\n",
      "Epoch 110/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.8188 - acc: 0.3521\n",
      "Epoch 111/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 2.7319 - acc: 0.3482\n",
      "Epoch 112/300\n",
      "11351/11351 [==============================] - 17s 1ms/step - loss: 2.6690 - acc: 0.3626\n",
      "Epoch 113/300\n",
      "11351/11351 [==============================] - 17s 1ms/step - loss: 2.6479 - acc: 0.3648\n",
      "Epoch 114/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.6303 - acc: 0.3741\n",
      "Epoch 115/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.6263 - acc: 0.3694\n",
      "Epoch 116/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.6206 - acc: 0.3763\n",
      "Epoch 117/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.6117 - acc: 0.3736\n",
      "Epoch 118/300\n",
      "11351/11351 [==============================] - 15s 1ms/step - loss: 2.5904 - acc: 0.3802\n",
      "Epoch 119/300\n",
      "11351/11351 [==============================] - 15s 1ms/step - loss: 2.5743 - acc: 0.3840\n",
      "Epoch 120/300\n",
      "11351/11351 [==============================] - 15s 1ms/step - loss: 2.5636 - acc: 0.3849\n",
      "Epoch 121/300\n",
      "11351/11351 [==============================] - 15s 1ms/step - loss: 2.5550 - acc: 0.3899\n",
      "Epoch 122/300\n",
      "11351/11351 [==============================] - 15s 1ms/step - loss: 2.5512 - acc: 0.3905\n",
      "Epoch 123/300\n",
      "11351/11351 [==============================] - 15s 1ms/step - loss: 2.5371 - acc: 0.3929\n",
      "Epoch 124/300\n",
      "11351/11351 [==============================] - 15s 1ms/step - loss: 2.5295 - acc: 0.3940\n",
      "Epoch 125/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.5204 - acc: 0.3964\n",
      "Epoch 126/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.5150 - acc: 0.3958\n",
      "Epoch 127/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.5036 - acc: 0.3990\n",
      "Epoch 128/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.4952 - acc: 0.3985\n",
      "Epoch 129/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.4787 - acc: 0.4021\n",
      "Epoch 130/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.4778 - acc: 0.4052\n",
      "Epoch 131/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.4750 - acc: 0.4005\n",
      "Epoch 132/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.4595 - acc: 0.4067\n",
      "Epoch 133/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.4552 - acc: 0.4047\n",
      "Epoch 134/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.4400 - acc: 0.4079\n",
      "Epoch 135/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.4299 - acc: 0.4137\n",
      "Epoch 136/300\n",
      "11351/11351 [==============================] - 17s 2ms/step - loss: 2.4370 - acc: 0.4079\n",
      "Epoch 137/300\n",
      "11351/11351 [==============================] - 17s 1ms/step - loss: 2.4144 - acc: 0.4196\n",
      "Epoch 138/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.4020 - acc: 0.4177\n",
      "Epoch 139/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.3987 - acc: 0.4206\n",
      "Epoch 140/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.3733 - acc: 0.4297\n",
      "Epoch 141/300\n",
      "11351/11351 [==============================] - 16s 1ms/step - loss: 2.3774 - acc: 0.4247\n",
      "Epoch 142/300\n",
      "11351/11351 [==============================] - 17s 1ms/step - loss: 2.3583 - acc: 0.4283\n",
      "Epoch 143/300\n",
      "11351/11351 [==============================] - 17s 1ms/step - loss: 2.3541 - acc: 0.4319\n",
      "Epoch 144/300\n",
      "11351/11351 [==============================] - 17s 1ms/step - loss: 2.3455 - acc: 0.4317\n",
      "Epoch 145/300\n",
      "11351/11351 [==============================] - 17s 1ms/step - loss: 2.3407 - acc: 0.4290\n",
      "Epoch 146/300\n",
      "11351/11351 [==============================] - 17s 1ms/step - loss: 2.3326 - acc: 0.4313\n",
      "Epoch 147/300\n",
      "11351/11351 [==============================] - 17s 1ms/step - loss: 2.3343 - acc: 0.4310\n",
      "Epoch 148/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.3168 - acc: 0.4367\n",
      "Epoch 149/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.3223 - acc: 0.4369\n",
      "Epoch 150/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.3255 - acc: 0.4338\n",
      "Epoch 151/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.2906 - acc: 0.4423\n",
      "Epoch 152/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.2655 - acc: 0.4489\n",
      "Epoch 153/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.2619 - acc: 0.4450\n",
      "Epoch 154/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.2613 - acc: 0.4508\n",
      "Epoch 155/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.2451 - acc: 0.4579\n",
      "Epoch 156/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.2393 - acc: 0.4556\n",
      "Epoch 157/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.2257 - acc: 0.4563\n",
      "Epoch 158/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.2261 - acc: 0.4559\n",
      "Epoch 159/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.2086 - acc: 0.4584\n",
      "Epoch 160/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.2169 - acc: 0.4571\n",
      "Epoch 161/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 2.2192 - acc: 0.4511\n",
      "Epoch 162/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 2.2009 - acc: 0.4583\n",
      "Epoch 163/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.1858 - acc: 0.4668\n",
      "Epoch 164/300\n",
      "11351/11351 [==============================] - 17s 2ms/step - loss: 2.1858 - acc: 0.4645\n",
      "Epoch 165/300\n",
      "11351/11351 [==============================] - 17s 2ms/step - loss: 2.1618 - acc: 0.4737\n",
      "Epoch 166/300\n",
      "11351/11351 [==============================] - 17s 2ms/step - loss: 2.1531 - acc: 0.4701\n",
      "Epoch 167/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 2.1516 - acc: 0.4733\n",
      "Epoch 168/300\n",
      "11351/11351 [==============================] - ETA: 0s - loss: 2.1371 - acc: 0.478 - 18s 2ms/step - loss: 2.1381 - acc: 0.4785\n",
      "Epoch 169/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.1381 - acc: 0.4746\n",
      "Epoch 170/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.1237 - acc: 0.4796\n",
      "Epoch 171/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 2.1076 - acc: 0.4802\n",
      "Epoch 172/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.1089 - acc: 0.4845\n",
      "Epoch 173/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.1179 - acc: 0.4774\n",
      "Epoch 174/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 2.0967 - acc: 0.4827\n",
      "Epoch 175/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 2.0972 - acc: 0.4874\n",
      "Epoch 176/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 2.0947 - acc: 0.4823\n",
      "Epoch 177/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 2.0864 - acc: 0.4870\n",
      "Epoch 178/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 2.0698 - acc: 0.4903\n",
      "Epoch 179/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 2.0481 - acc: 0.4964\n",
      "Epoch 180/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 2.0419 - acc: 0.4967\n",
      "Epoch 181/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 2.0239 - acc: 0.4989\n",
      "Epoch 182/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 2.0236 - acc: 0.4987\n",
      "Epoch 183/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 2.0206 - acc: 0.5014\n",
      "Epoch 184/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 2.0075 - acc: 0.5029\n",
      "Epoch 185/300\n",
      "11351/11351 [==============================] - 22s 2ms/step - loss: 1.9964 - acc: 0.5096\n",
      "Epoch 186/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.9867 - acc: 0.5085\n",
      "Epoch 187/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.9912 - acc: 0.5071\n",
      "Epoch 188/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.9671 - acc: 0.5095\n",
      "Epoch 189/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.9697 - acc: 0.5113\n",
      "Epoch 190/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.9608 - acc: 0.5127\n",
      "Epoch 191/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.9567 - acc: 0.5141\n",
      "Epoch 192/300\n",
      "11351/11351 [==============================] - 23s 2ms/step - loss: 1.9463 - acc: 0.5160\n",
      "Epoch 193/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.9394 - acc: 0.5183\n",
      "Epoch 194/300\n",
      "11351/11351 [==============================] - 25s 2ms/step - loss: 1.9313 - acc: 0.5170\n",
      "Epoch 195/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.9151 - acc: 0.5228\n",
      "Epoch 196/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 1.9068 - acc: 0.5260\n",
      "Epoch 197/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 1.9099 - acc: 0.5253\n",
      "Epoch 198/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 1.9367 - acc: 0.5179\n",
      "Epoch 199/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 1.9085 - acc: 0.5269\n",
      "Epoch 200/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 1.9103 - acc: 0.5243\n",
      "Epoch 201/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 1.8784 - acc: 0.5334\n",
      "Epoch 202/300\n",
      "11351/11351 [==============================] - 18s 2ms/step - loss: 1.8654 - acc: 0.5374\n",
      "Epoch 203/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.8563 - acc: 0.5355\n",
      "Epoch 204/300\n",
      "11351/11351 [==============================] - 25s 2ms/step - loss: 1.8459 - acc: 0.5451\n",
      "Epoch 205/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 1.8324 - acc: 0.5472\n",
      "Epoch 206/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.8358 - acc: 0.5422\n",
      "Epoch 207/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.8566 - acc: 0.5302\n",
      "Epoch 208/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 1.8420 - acc: 0.5375\n",
      "Epoch 209/300\n",
      "11351/11351 [==============================] - 19s 2ms/step - loss: 1.8145 - acc: 0.5481\n",
      "Epoch 210/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.8013 - acc: 0.5500\n",
      "Epoch 211/300\n",
      "11351/11351 [==============================] - 24s 2ms/step - loss: 1.8125 - acc: 0.5462\n",
      "Epoch 212/300\n",
      "11351/11351 [==============================] - 23s 2ms/step - loss: 1.7865 - acc: 0.5531\n",
      "Epoch 213/300\n",
      "11351/11351 [==============================] - 26s 2ms/step - loss: 1.7868 - acc: 0.5512\n",
      "Epoch 214/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.8067 - acc: 0.5448\n",
      "Epoch 215/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.7956 - acc: 0.5492\n",
      "Epoch 216/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.7742 - acc: 0.5554\n",
      "Epoch 217/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.7573 - acc: 0.5576\n",
      "Epoch 218/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.7809 - acc: 0.5542\n",
      "Epoch 219/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.7497 - acc: 0.5622\n",
      "Epoch 220/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.7269 - acc: 0.5672\n",
      "Epoch 221/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.7301 - acc: 0.5643\n",
      "Epoch 222/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.7313 - acc: 0.5637\n",
      "Epoch 223/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.7082 - acc: 0.5737\n",
      "Epoch 224/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6990 - acc: 0.5754\n",
      "Epoch 225/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6899 - acc: 0.5778\n",
      "Epoch 226/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6866 - acc: 0.5784\n",
      "Epoch 227/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6774 - acc: 0.5821\n",
      "Epoch 228/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6864 - acc: 0.5789\n",
      "Epoch 229/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6793 - acc: 0.5765\n",
      "Epoch 230/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6621 - acc: 0.5805\n",
      "Epoch 231/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.6571 - acc: 0.5821\n",
      "Epoch 232/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6559 - acc: 0.5845\n",
      "Epoch 233/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6373 - acc: 0.5883\n",
      "Epoch 234/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6295 - acc: 0.5917\n",
      "Epoch 235/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6358 - acc: 0.5856\n",
      "Epoch 236/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6424 - acc: 0.5826\n",
      "Epoch 237/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6399 - acc: 0.5922\n",
      "Epoch 238/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6366 - acc: 0.5850\n",
      "Epoch 239/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6067 - acc: 0.5977\n",
      "Epoch 240/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.5966 - acc: 0.5980\n",
      "Epoch 241/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.5904 - acc: 0.6025\n",
      "Epoch 242/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.6010 - acc: 0.5932\n",
      "Epoch 243/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.5781 - acc: 0.6016\n",
      "Epoch 244/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.5701 - acc: 0.6036\n",
      "Epoch 245/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.5725 - acc: 0.6048\n",
      "Epoch 246/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.5655 - acc: 0.6040\n",
      "Epoch 247/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.5645 - acc: 0.6024\n",
      "Epoch 248/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.5589 - acc: 0.6091\n",
      "Epoch 249/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.5512 - acc: 0.6095\n",
      "Epoch 250/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.5447 - acc: 0.6073\n",
      "Epoch 251/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.5281 - acc: 0.6155\n",
      "Epoch 252/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.5181 - acc: 0.6189\n",
      "Epoch 253/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4948 - acc: 0.6275\n",
      "Epoch 254/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4889 - acc: 0.6223\n",
      "Epoch 255/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4876 - acc: 0.6217\n",
      "Epoch 256/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.4844 - acc: 0.6251\n",
      "Epoch 257/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4832 - acc: 0.6241\n",
      "Epoch 258/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4893 - acc: 0.6254\n",
      "Epoch 259/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4921 - acc: 0.6251\n",
      "Epoch 260/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.4874 - acc: 0.6216\n",
      "Epoch 261/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4713 - acc: 0.6253\n",
      "Epoch 262/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4827 - acc: 0.6273\n",
      "Epoch 263/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4880 - acc: 0.6245\n",
      "Epoch 264/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4567 - acc: 0.6316\n",
      "Epoch 265/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4431 - acc: 0.6364\n",
      "Epoch 266/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4306 - acc: 0.6402\n",
      "Epoch 267/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4332 - acc: 0.6386\n",
      "Epoch 268/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4194 - acc: 0.6438\n",
      "Epoch 269/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.3989 - acc: 0.6492\n",
      "Epoch 270/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.4052 - acc: 0.6502\n",
      "Epoch 271/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3982 - acc: 0.6473\n",
      "Epoch 272/300\n",
      "11351/11351 [==============================] - 20s 2ms/step - loss: 1.3936 - acc: 0.6490\n",
      "Epoch 273/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3961 - acc: 0.6473\n",
      "Epoch 274/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3875 - acc: 0.6487\n",
      "Epoch 275/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3870 - acc: 0.6471\n",
      "Epoch 276/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3922 - acc: 0.6470\n",
      "Epoch 277/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3783 - acc: 0.6480\n",
      "Epoch 278/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3760 - acc: 0.6498\n",
      "Epoch 279/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3626 - acc: 0.6571\n",
      "Epoch 280/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3498 - acc: 0.6626\n",
      "Epoch 281/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3375 - acc: 0.6653\n",
      "Epoch 282/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3350 - acc: 0.6653\n",
      "Epoch 283/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3356 - acc: 0.6665\n",
      "Epoch 284/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3501 - acc: 0.6589\n",
      "Epoch 285/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3572 - acc: 0.6574\n",
      "Epoch 286/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3326 - acc: 0.6651\n",
      "Epoch 287/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3156 - acc: 0.6668\n",
      "Epoch 288/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.3150 - acc: 0.6703\n",
      "Epoch 289/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.2969 - acc: 0.6710\n",
      "Epoch 290/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.2858 - acc: 0.6773\n",
      "Epoch 291/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.2672 - acc: 0.6822\n",
      "Epoch 292/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.2572 - acc: 0.6840\n",
      "Epoch 293/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.2646 - acc: 0.6798\n",
      "Epoch 294/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.2680 - acc: 0.6760\n",
      "Epoch 295/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.2797 - acc: 0.6769\n",
      "Epoch 296/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.2756 - acc: 0.6778\n",
      "Epoch 297/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.2630 - acc: 0.6806\n",
      "Epoch 298/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.2484 - acc: 0.6834\n",
      "Epoch 299/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.2340 - acc: 0.6903\n",
      "Epoch 300/300\n",
      "11351/11351 [==============================] - 21s 2ms/step - loss: 1.2201 - acc: 0.6950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1738a7f7a88>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,batch_size=150, epochs=300, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_mobydick_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer, open('my_firsttokenizer', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text (model, tokenizer, seq_len, seed_text, num_gen_words ):\n",
    "    \n",
    "    output_text = []\n",
    "    input_text = seed_text # starting text for the model for prediction\n",
    "    \n",
    "    \n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0] #same step we did for converting text to sequences\n",
    "        \n",
    "        pad_encoded  = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre') # to incorporate unbalanced text\n",
    "        \n",
    "        pred_word_index = model.predict_classes(pad_encoded, verbose=0)[0] # index value of the words from the predicted values\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_index] # actual word from the vocabulary to get the english word\n",
    "        \n",
    "        input_text += ' ' + pred_word # finally the text is feeded into \n",
    "        \n",
    "        output_text.append(pred_word) # output text is a list of strings and we join it to make a sentence\n",
    "        \n",
    "    return ' '.join(output_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(text_seq[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i thought'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i would sail at their strange inn ashore to myself with the room and somehow and then n't see a word that being bed the\""
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer,seq_len, seed_text=seed_text, num_gen_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
