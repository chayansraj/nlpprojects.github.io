{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download_shell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5   WordNet\n",
    "WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155,287 words and 117,659 synonym sets. We'll begin by looking at synonyms and how they are accessed in WordNet.\n",
    "\n",
    "5.1   Senses and Synonyms\n",
    "Consider the sentence in (1a). If we replace the word motorcar in (1a) by automobile, to get (1b), the meaning of the sentence stays pretty much the same:\n",
    "\n",
    "(1)\t\t\n",
    "a.\t\tBenz is credited with the invention of the motorcar.\n",
    "\n",
    "b.\t\tBenz is credited with the invention of the automobile.\n",
    "\n",
    "\n",
    "Since everything else in the sentence has remained unchanged, we can conclude that the words motorcar and automobile have the same meaning, i.e. they are synonyms. We can explore these words with the help of WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('death.n.01'),\n",
       " Synset('death.n.02'),\n",
       " Synset('death.n.03'),\n",
       " Synset('death.n.04'),\n",
       " Synset('death.n.05'),\n",
       " Synset('death.n.06'),\n",
       " Synset('end.n.06'),\n",
       " Synset('death.n.08')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('death')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love'] ['love', 'passion'] ['beloved', 'dear', 'dearest', 'honey', 'love'] ['love', 'sexual_love', 'erotic_love'] ['love'] ['sexual_love', 'lovemaking', 'making_love', 'love', 'love_life'] ['love'] ['love', 'enjoy'] ['love'] ['sleep_together', 'roll_in_the_hay', 'love', 'make_out', 'make_love', 'sleep_with', 'get_laid', 'have_sex', 'know', 'do_it', 'be_intimate', 'have_intercourse', 'have_it_away', 'have_it_off', 'screw', 'fuck', 'jazz', 'eff', 'hump', 'lie_with', 'bed', 'have_a_go_at_it', 'bang', 'get_it_on', 'bonk'] "
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets('love'):\n",
    "    print(syn.lemma_names(), end = ' ')\n",
    "\n",
    "#wn.synset('death.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('fire.n.01'),\n",
       " Synset('fire.n.02'),\n",
       " Synset('fire.n.03'),\n",
       " Synset('fire.n.04'),\n",
       " Synset('fire.n.05'),\n",
       " Synset('ardor.n.03'),\n",
       " Synset('fire.n.07'),\n",
       " Synset('fire.n.08'),\n",
       " Synset('fire.n.09'),\n",
       " Synset('open_fire.v.01'),\n",
       " Synset('fire.v.02'),\n",
       " Synset('fire.v.03'),\n",
       " Synset('displace.v.03'),\n",
       " Synset('fire.v.05'),\n",
       " Synset('fire.v.06'),\n",
       " Synset('arouse.v.01'),\n",
       " Synset('burn.v.01'),\n",
       " Synset('fuel.v.02')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('fire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('fire.n.01') [Lemma('fire.n.01.fire')] the event of something burning (often destructive)\n",
      "\n",
      "Synset('fire.n.02') [Lemma('fire.n.02.fire'), Lemma('fire.n.02.firing')] the act of firing weapons or artillery at an enemy\n",
      "\n",
      "Synset('fire.n.03') [Lemma('fire.n.03.fire'), Lemma('fire.n.03.flame'), Lemma('fire.n.03.flaming')] the process of combustion of inflammable materials producing heat and light and (often) smoke\n",
      "\n",
      "Synset('fire.n.04') [Lemma('fire.n.04.fire')] a fireplace in which a relatively small fire is burning\n",
      "\n",
      "Synset('fire.n.05') [Lemma('fire.n.05.fire')] once thought to be one of four elements composing the universe (Empedocles)\n",
      "\n",
      "Synset('ardor.n.03') [Lemma('ardor.n.03.ardor'), Lemma('ardor.n.03.ardour'), Lemma('ardor.n.03.fervor'), Lemma('ardor.n.03.fervour'), Lemma('ardor.n.03.fervency'), Lemma('ardor.n.03.fire'), Lemma('ardor.n.03.fervidness')] feelings of great warmth and intensity\n",
      "\n",
      "Synset('fire.n.07') [Lemma('fire.n.07.fire')] fuel that is burning and is used as a means for cooking\n",
      "\n",
      "Synset('fire.n.08') [Lemma('fire.n.08.fire')] a severe trial\n",
      "\n",
      "Synset('fire.n.09') [Lemma('fire.n.09.fire'), Lemma('fire.n.09.attack'), Lemma('fire.n.09.flak'), Lemma('fire.n.09.flack'), Lemma('fire.n.09.blast')] intense adverse criticism\n",
      "\n",
      "Synset('open_fire.v.01') [Lemma('open_fire.v.01.open_fire'), Lemma('open_fire.v.01.fire')] start firing a weapon\n",
      "\n",
      "Synset('fire.v.02') [Lemma('fire.v.02.fire'), Lemma('fire.v.02.discharge')] cause to go off\n",
      "\n",
      "Synset('fire.v.03') [Lemma('fire.v.03.fire')] bake in a kiln so as to harden\n",
      "\n",
      "Synset('displace.v.03') [Lemma('displace.v.03.displace'), Lemma('displace.v.03.fire'), Lemma('displace.v.03.give_notice'), Lemma('displace.v.03.can'), Lemma('displace.v.03.dismiss'), Lemma('displace.v.03.give_the_axe'), Lemma('displace.v.03.send_away'), Lemma('displace.v.03.sack'), Lemma('displace.v.03.force_out'), Lemma('displace.v.03.give_the_sack'), Lemma('displace.v.03.terminate')] terminate the employment of; discharge from an office or position\n",
      "\n",
      "Synset('fire.v.05') [Lemma('fire.v.05.fire'), Lemma('fire.v.05.discharge'), Lemma('fire.v.05.go_off')] go off or discharge\n",
      "\n",
      "Synset('fire.v.06') [Lemma('fire.v.06.fire')] drive out or away by or as if by fire\n",
      "\n",
      "Synset('arouse.v.01') [Lemma('arouse.v.01.arouse'), Lemma('arouse.v.01.elicit'), Lemma('arouse.v.01.enkindle'), Lemma('arouse.v.01.kindle'), Lemma('arouse.v.01.evoke'), Lemma('arouse.v.01.fire'), Lemma('arouse.v.01.raise'), Lemma('arouse.v.01.provoke')] call forth (emotions, feelings, and responses)\n",
      "\n",
      "Synset('burn.v.01') [Lemma('burn.v.01.burn'), Lemma('burn.v.01.fire'), Lemma('burn.v.01.burn_down')] destroy by fire\n",
      "\n",
      "Synset('fuel.v.02') [Lemma('fuel.v.02.fuel'), Lemma('fuel.v.02.fire')] provide with fuel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets('fire'):\n",
    "    print( syn , syn.lemmas(), syn.definition(), end ='\\n\\n')\n",
    "\n",
    "#wn.synset('fire.n.03').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"fire was one of our ancestors' first discoveries\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('fire.n.03').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('love.n.01'),\n",
       " Synset('love.n.02'),\n",
       " Synset('beloved.n.01'),\n",
       " Synset('love.n.04'),\n",
       " Synset('love.n.05'),\n",
       " Synset('sexual_love.n.02'),\n",
       " Synset('love.v.01'),\n",
       " Synset('love.v.02'),\n",
       " Synset('love.v.03'),\n",
       " Synset('sleep_together.v.01')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('love.n.04.love'),\n",
       " Lemma('love.n.04.sexual_love'),\n",
       " Lemma('love.n.04.erotic_love')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('love.n.04').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love']\n",
      "['love', 'passion']\n",
      "['beloved', 'dear', 'dearest', 'honey', 'love']\n",
      "['love', 'sexual_love', 'erotic_love']\n",
      "['love']\n",
      "['sexual_love', 'lovemaking', 'making_love', 'love', 'love_life']\n",
      "['love']\n",
      "['love', 'enjoy']\n",
      "['love']\n",
      "['sleep_together', 'roll_in_the_hay', 'love', 'make_out', 'make_love', 'sleep_with', 'get_laid', 'have_sex', 'know', 'do_it', 'be_intimate', 'have_intercourse', 'have_it_away', 'have_it_off', 'screw', 'fuck', 'jazz', 'eff', 'hump', 'lie_with', 'bed', 'have_a_go_at_it', 'bang', 'get_it_on', 'bonk']\n"
     ]
    }
   ],
   "source": [
    "for sys in wn.synsets('love'):\n",
    "    print(sys.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('emotion.n.01')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('love.n.01').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar = wn.synset('car.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('ambulance.n.01')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('ambulance.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('step.v.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('walk.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chew.v.01'), Synset('swallow.v.01')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('eat.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = wn.synset('right_whale.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca = wn.synset('orca.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('whale.n.02')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(orca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('whale.n.02.whale')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('whale.n.02').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('death.n.01'),\n",
       " Synset('death.n.02'),\n",
       " Synset('death.n.03'),\n",
       " Synset('death.n.04'),\n",
       " Synset('death.n.05'),\n",
       " Synset('death.n.06'),\n",
       " Synset('end.n.06'),\n",
       " Synset('death.n.08')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('death')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['death', 'dying', 'demise']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('death.n.04').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "death = wn.synset('death.n.02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = wn.synset('death.n.04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "death.lowest_common_hypernyms(dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstraction', 'abstract_entity']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('abstraction.n.06').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08333333333333333"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "death.path_similarity(dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on WordNetCorpusReader in module nltk.corpus.reader.wordnet object:\n",
      "\n",
      "class WordNetCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      " |  WordNetCorpusReader(root, omw_reader)\n",
      " |  \n",
      " |  A corpus reader used to access wordnet or its variants.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      WordNetCorpusReader\n",
      " |      nltk.corpus.reader.api.CorpusReader\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, root, omw_reader)\n",
      " |      Construct a new wordnet corpus reader, with the given root\n",
      " |      directory.\n",
      " |  \n",
      " |  all_lemma_names(self, pos=None, lang='eng')\n",
      " |      Return all lemma names for all synsets for the given\n",
      " |      part of speech tag and language or languages. If pos is\n",
      " |      not specified, all synsets for all parts of speech will\n",
      " |      be used.\n",
      " |  \n",
      " |  all_synsets(self, pos=None)\n",
      " |      Iterate over all synsets with a given part of speech tag.\n",
      " |      If no pos is specified, all synsets for all parts of speech\n",
      " |      will be loaded.\n",
      " |  \n",
      " |  citation(self, lang='omw')\n",
      " |      Return the contents of citation.bib file (for omw)\n",
      " |      use lang=lang to get the citation for an individual language\n",
      " |  \n",
      " |  custom_lemmas(self, tab_file, lang)\n",
      " |      Reads a custom tab file containing mappings of lemmas in the given\n",
      " |      language to Princeton WordNet 3.0 synset offsets, allowing NLTK's\n",
      " |      WordNet functions to then be used with that language.\n",
      " |      \n",
      " |      See the \"Tab files\" section at http://compling.hss.ntu.edu.sg/omw/ for\n",
      " |      documentation on the Multilingual WordNet tab file format.\n",
      " |      \n",
      " |      :param tab_file: Tab file as a file or file-like object\n",
      " |      :type  lang str\n",
      " |      :param lang ISO 639-3 code of the language of the tab file\n",
      " |  \n",
      " |  get_version(self)\n",
      " |  \n",
      " |  ic(self, corpus, weight_senses_equally=False, smoothing=1.0)\n",
      " |      Creates an information content lookup dictionary from a corpus.\n",
      " |      \n",
      " |      :type corpus: CorpusReader\n",
      " |      :param corpus: The corpus from which we create an information\n",
      " |      content dictionary.\n",
      " |      :type weight_senses_equally: bool\n",
      " |      :param weight_senses_equally: If this is True, gives all\n",
      " |      possible senses equal weight rather than dividing by the\n",
      " |      number of possible senses.  (If a word has 3 synses, each\n",
      " |      sense gets 0.3333 per appearance when this is False, 1.0 when\n",
      " |      it is true.)\n",
      " |      :param smoothing: How much do we smooth synset counts (default is 1.0)\n",
      " |      :type smoothing: float\n",
      " |      :return: An information content dictionary\n",
      " |  \n",
      " |  jcn_similarity(self, synset1, synset2, ic, verbose=False)\n",
      " |      Jiang-Conrath Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      " |      ancestor node) and that of the two input Synsets. The relationship is\n",
      " |      given by the equation 1 / (IC(s1) + IC(s2) - 2 * IC(lcs)).\n",
      " |      \n",
      " |      :type  other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type  ic: dict\n",
      " |      :param ic: an information content object (as returned by\n",
      " |          ``nltk.corpus.wordnet_ic.ic()``).\n",
      " |      :return: A float score denoting the similarity of the two ``Synset``\n",
      " |          objects.\n",
      " |  \n",
      " |  langs(self)\n",
      " |      return a list of languages supported by Multilingual Wordnet\n",
      " |  \n",
      " |  lch_similarity(self, synset1, synset2, verbose=False, simulate_root=True)\n",
      " |      Leacock Chodorow Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      shortest path that connects the senses (as above) and the maximum depth\n",
      " |      of the taxonomy in which the senses occur. The relationship is given as\n",
      " |      -log(p/2d) where p is the shortest path length and d is the taxonomy\n",
      " |      depth.\n",
      " |      \n",
      " |      :type  other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type simulate_root: bool\n",
      " |      :param simulate_root: The various verb taxonomies do not\n",
      " |          share a single root which disallows this metric from working for\n",
      " |          synsets that are not connected. This flag (True by default)\n",
      " |          creates a fake root that connects all the taxonomies. Set it\n",
      " |          to false to disable this behavior. For the noun taxonomy,\n",
      " |          there is usually a default root except for WordNet version 1.6.\n",
      " |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      " |          as well.\n",
      " |      :return: A score denoting the similarity of the two ``Synset`` objects,\n",
      " |          normally greater than 0. None is returned if no connecting path\n",
      " |          could be found. If a ``Synset`` is compared with itself, the\n",
      " |          maximum score is returned, which varies depending on the taxonomy\n",
      " |          depth.\n",
      " |  \n",
      " |  lemma(self, name, lang='eng')\n",
      " |      Return lemma object that matches the name\n",
      " |  \n",
      " |  lemma_count(self, lemma)\n",
      " |      Return the frequency count for this Lemma\n",
      " |  \n",
      " |  lemma_from_key(self, key)\n",
      " |  \n",
      " |  lemmas(self, lemma, pos=None, lang='eng')\n",
      " |      Return all Lemma objects with a name matching the specified lemma\n",
      " |      name and part of speech tag. Matches any part of speech tag if none is\n",
      " |      specified.\n",
      " |  \n",
      " |  license(self, lang='eng')\n",
      " |      Return the contents of LICENSE (for omw)\n",
      " |      use lang=lang to get the license for an individual language\n",
      " |  \n",
      " |  lin_similarity(self, synset1, synset2, ic, verbose=False)\n",
      " |      Lin Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      " |      ancestor node) and that of the two input Synsets. The relationship is\n",
      " |      given by the equation 2 * IC(lcs) / (IC(s1) + IC(s2)).\n",
      " |      \n",
      " |      :type other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type ic: dict\n",
      " |      :param ic: an information content object (as returned by\n",
      " |          ``nltk.corpus.wordnet_ic.ic()``).\n",
      " |      :return: A float score denoting the similarity of the two ``Synset``\n",
      " |          objects, in the range 0 to 1.\n",
      " |  \n",
      " |  morphy(self, form, pos=None, check_exceptions=True)\n",
      " |      Find a possible base form for the given form, with the given\n",
      " |      part of speech, by checking WordNet's list of exceptional\n",
      " |      forms, and by recursively stripping affixes for this part of\n",
      " |      speech until a form in WordNet is found.\n",
      " |      \n",
      " |      >>> from nltk.corpus import wordnet as wn\n",
      " |      >>> print(wn.morphy('dogs'))\n",
      " |      dog\n",
      " |      >>> print(wn.morphy('churches'))\n",
      " |      church\n",
      " |      >>> print(wn.morphy('aardwolves'))\n",
      " |      aardwolf\n",
      " |      >>> print(wn.morphy('abaci'))\n",
      " |      abacus\n",
      " |      >>> wn.morphy('hardrock', wn.ADV)\n",
      " |      >>> print(wn.morphy('book', wn.NOUN))\n",
      " |      book\n",
      " |      >>> wn.morphy('book', wn.ADJ)\n",
      " |  \n",
      " |  of2ss(self, of)\n",
      " |      take an id and return the synsets\n",
      " |  \n",
      " |  path_similarity(self, synset1, synset2, verbose=False, simulate_root=True)\n",
      " |      Path Distance Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      shortest path that connects the senses in the is-a (hypernym/hypnoym)\n",
      " |      taxonomy. The score is in the range 0 to 1, except in those cases where\n",
      " |      a path cannot be found (will only be true for verbs as there are many\n",
      " |      distinct verb taxonomies), in which case None is returned. A score of\n",
      " |      1 represents identity i.e. comparing a sense with itself will return 1.\n",
      " |      \n",
      " |      :type other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type simulate_root: bool\n",
      " |      :param simulate_root: The various verb taxonomies do not\n",
      " |          share a single root which disallows this metric from working for\n",
      " |          synsets that are not connected. This flag (True by default)\n",
      " |          creates a fake root that connects all the taxonomies. Set it\n",
      " |          to false to disable this behavior. For the noun taxonomy,\n",
      " |          there is usually a default root except for WordNet version 1.6.\n",
      " |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      " |          as well.\n",
      " |      :return: A score denoting the similarity of the two ``Synset`` objects,\n",
      " |          normally between 0 and 1. None is returned if no connecting path\n",
      " |          could be found. 1 is returned if a ``Synset`` is compared with\n",
      " |          itself.\n",
      " |  \n",
      " |  readme(self, lang='omw')\n",
      " |      Return the contents of README (for omw)\n",
      " |      use lang=lang to get the readme for an individual language\n",
      " |  \n",
      " |  res_similarity(self, synset1, synset2, ic, verbose=False)\n",
      " |      Resnik Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      " |      ancestor node).\n",
      " |      \n",
      " |      :type  other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type ic: dict\n",
      " |      :param ic: an information content object (as returned by\n",
      " |          ``nltk.corpus.wordnet_ic.ic()``).\n",
      " |      :return: A float score denoting the similarity of the two ``Synset``\n",
      " |          objects. Synsets whose LCS is the root node of the taxonomy will\n",
      " |          have a score of 0 (e.g. N['dog'][0] and N['table'][0]).\n",
      " |  \n",
      " |  ss2of(self, ss, lang=None)\n",
      " |      return the ID of the synset\n",
      " |  \n",
      " |  synset(self, name)\n",
      " |      #############################################################\n",
      " |      # Loading Synsets\n",
      " |      #############################################################\n",
      " |  \n",
      " |  synset_from_pos_and_offset(self, pos, offset)\n",
      " |  \n",
      " |  synset_from_sense_key(self, sense_key)\n",
      " |      Retrieves synset based on a given sense_key. Sense keys can be\n",
      " |      obtained from lemma.key()\n",
      " |      \n",
      " |      From https://wordnet.princeton.edu/wordnet/man/senseidx.5WN.html:\n",
      " |      A sense_key is represented as:\n",
      " |          lemma % lex_sense (e.g. 'dog%1:18:01::')\n",
      " |      where lex_sense is encoded as:\n",
      " |          ss_type:lex_filenum:lex_id:head_word:head_id\n",
      " |      \n",
      " |      lemma:       ASCII text of word/collocation, in lower case\n",
      " |      ss_type:     synset type for the sense (1 digit int)\n",
      " |                   The synset type is encoded as follows:\n",
      " |                   1    NOUN\n",
      " |                   2    VERB\n",
      " |                   3    ADJECTIVE\n",
      " |                   4    ADVERB\n",
      " |                   5    ADJECTIVE SATELLITE\n",
      " |      lex_filenum: name of lexicographer file containing the synset for the sense (2 digit int)\n",
      " |      lex_id:      when paired with lemma, uniquely identifies a sense in the lexicographer file (2 digit int)\n",
      " |      head_word:   lemma of the first word in satellite's head synset\n",
      " |                   Only used if sense is in an adjective satellite synset\n",
      " |      head_id:     uniquely identifies sense in a lexicographer file when paired with head_word\n",
      " |                   Only used if head_word is present (2 digit int)\n",
      " |  \n",
      " |  synsets(self, lemma, pos=None, lang='eng', check_exceptions=True)\n",
      " |      Load all synsets with a given lemma and part of speech tag.\n",
      " |      If no pos is specified, all synsets for all parts of speech\n",
      " |      will be loaded.\n",
      " |      If lang is specified, all the synsets associated with the lemma name\n",
      " |      of that language will be returned.\n",
      " |  \n",
      " |  words(self, lang='eng')\n",
      " |      return lemmas of the given language as list of words\n",
      " |  \n",
      " |  wup_similarity(self, synset1, synset2, verbose=False, simulate_root=True)\n",
      " |      Wu-Palmer Similarity:\n",
      " |      Return a score denoting how similar two word senses are, based on the\n",
      " |      depth of the two senses in the taxonomy and that of their Least Common\n",
      " |      Subsumer (most specific ancestor node). Previously, the scores computed\n",
      " |      by this implementation did _not_ always agree with those given by\n",
      " |      Pedersen's Perl implementation of WordNet Similarity. However, with\n",
      " |      the addition of the simulate_root flag (see below), the score for\n",
      " |      verbs now almost always agree but not always for nouns.\n",
      " |      \n",
      " |      The LCS does not necessarily feature in the shortest path connecting\n",
      " |      the two senses, as it is by definition the common ancestor deepest in\n",
      " |      the taxonomy, not closest to the two senses. Typically, however, it\n",
      " |      will so feature. Where multiple candidates for the LCS exist, that\n",
      " |      whose shortest path to the root node is the longest will be selected.\n",
      " |      Where the LCS has multiple paths to the root, the longer path is used\n",
      " |      for the purposes of the calculation.\n",
      " |      \n",
      " |      :type  other: Synset\n",
      " |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      " |      :type simulate_root: bool\n",
      " |      :param simulate_root: The various verb taxonomies do not\n",
      " |          share a single root which disallows this metric from working for\n",
      " |          synsets that are not connected. This flag (True by default)\n",
      " |          creates a fake root that connects all the taxonomies. Set it\n",
      " |          to false to disable this behavior. For the noun taxonomy,\n",
      " |          there is usually a default root except for WordNet version 1.6.\n",
      " |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      " |          as well.\n",
      " |      :return: A float score denoting the similarity of the two ``Synset``\n",
      " |          objects, normally greater than zero. If no connecting path between\n",
      " |          the two senses can be found, None is returned.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ADJ = 'a'\n",
      " |  \n",
      " |  ADJ_SAT = 's'\n",
      " |  \n",
      " |  ADV = 'r'\n",
      " |  \n",
      " |  MORPHOLOGICAL_SUBSTITUTIONS = {'a': [('er', ''), ('est', ''), ('er', '...\n",
      " |  \n",
      " |  NOUN = 'n'\n",
      " |  \n",
      " |  VERB = 'v'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |  \n",
      " |  abspath(self, fileid)\n",
      " |      Return the absolute path for the given file.\n",
      " |      \n",
      " |      :type fileid: str\n",
      " |      :param fileid: The file identifier for the file whose path\n",
      " |          should be returned.\n",
      " |      :rtype: PathPointer\n",
      " |  \n",
      " |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      " |      Return a list of the absolute paths for all fileids in this corpus;\n",
      " |      or for the given list of fileids, if specified.\n",
      " |      \n",
      " |      :type fileids: None or str or list\n",
      " |      :param fileids: Specifies the set of fileids for which paths should\n",
      " |          be returned.  Can be None, for all fileids; a list of\n",
      " |          file identifiers, for a specified set of fileids; or a single\n",
      " |          file identifier, for a single file.  Note that the return\n",
      " |          value is always a list of paths, even if ``fileids`` is a\n",
      " |          single file identifier.\n",
      " |      \n",
      " |      :param include_encoding: If true, then return a list of\n",
      " |          ``(path_pointer, encoding)`` tuples.\n",
      " |      \n",
      " |      :rtype: list(PathPointer)\n",
      " |  \n",
      " |  encoding(self, file)\n",
      " |      Return the unicode encoding for the given corpus file, if known.\n",
      " |      If the encoding is unknown, or if the given file should be\n",
      " |      processed using byte strings (str), then return None.\n",
      " |  \n",
      " |  ensure_loaded(self)\n",
      " |      Load this corpus (if it has not already been loaded).  This is\n",
      " |      used by LazyCorpusLoader as a simple method that can be used to\n",
      " |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      " |      do help(some_corpus).\n",
      " |  \n",
      " |  fileids(self)\n",
      " |      Return a list of file identifiers for the fileids that make up\n",
      " |      this corpus.\n",
      " |  \n",
      " |  open(self, file)\n",
      " |      Return an open stream that can be used to read the given file.\n",
      " |      If the file's encoding is not None, then the stream will\n",
      " |      automatically decode the file's contents into unicode.\n",
      " |      \n",
      " |      :param file: The file identifier of the file to read.\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  root\n",
      " |      The directory where this corpus is stored.\n",
      " |      \n",
      " |      :type: PathPointer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jin_si\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement jin_si (from versions: )\n",
      "No matching distribution found for jin_si\n"
     ]
    }
   ],
   "source": [
    "!pip install jin_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.01.auto'),\n",
       " Lemma('car.n.01.automobile'),\n",
       " Lemma('car.n.01.machine'),\n",
       " Lemma('car.n.01.motorcar')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car car\n",
      "car railcar\n",
      "car railway_car\n",
      "car railroad_car\n",
      "auto car\n",
      "auto railcar\n",
      "auto railway_car\n",
      "auto railroad_car\n",
      "automobile car\n",
      "automobile railcar\n",
      "automobile railway_car\n",
      "automobile railroad_car\n",
      "machine car\n",
      "machine railcar\n",
      "machine railway_car\n",
      "machine railroad_car\n",
      "motorcar car\n",
      "motorcar railcar\n",
      "motorcar railway_car\n",
      "motorcar railroad_car\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for syn in wn.synsets('car')[0].lemma_names():\n",
    "    for syn1 in wn.synsets('car')[1].lemma_names():\n",
    "        print(syn,   syn1,  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'railcar', 'railway_car', 'railroad_car']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('car')[1].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7272727272727273"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.wup_similarity(wn.synsets('car')[0] , wn.synsets('car')[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize , pos_tag\n",
    "\n",
    "    \n",
    "#function to convert part of speech tag according to the wordnet context\n",
    "def to_wordnet_tag(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return None\n",
    "\n",
    "\n",
    "    #function to find the synonymous set of the tagged words in the sentences\n",
    "def tag_to_syn(word, tag):\n",
    "    wn_tag = to_wordnet_tag(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    "    try:\n",
    "        return wn.synsets(word,wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    #function to find the similarity between the sentences using wordnet\n",
    "def sent_similarity(text1,text2):\n",
    "    score, count = 0.0,0\n",
    "\n",
    "    #first tokenize the text and tag words with part of speech tagging\n",
    "    text1 = pos_tag(word_tokenize(text1))\n",
    "    text2 = pos_tag(word_tokenize(text2))\n",
    "\n",
    "    #Now get the synonymous set of all the tagged words\n",
    "    syns1 = [tag_to_syn(*tag_words) for tag_words in text1]\n",
    "    syns2 = [tag_to_syn(*tag_words) for tag_words in text2]\n",
    "\n",
    "    #We don't want any noise in the synsets so we remove Nones' from the sysnets\n",
    "    synset1 = [s for s in syns1 if s ]\n",
    "    synset2 = [s for s in syns2 if s ]\n",
    "\n",
    "    # For each word in the first sentence\n",
    "    for syn1 in synset1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score=[\n",
    "            wn.wup_similarity(syn1, syn2)\n",
    "            if not isinstance(synset1, str) and not isinstance(synset2, str)\n",
    "            # just in case there are scientific words wordnet does not have\n",
    "            else fuzz.WRatio(str(synset1), str(synset2)) / 100\n",
    "            for syn2 in synset2\n",
    "        ]\n",
    "        best_score=[s if s else 0 for s in best_score]\n",
    "        # print(synsets1, synsets2)\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score:\n",
    "            score += max(best_score)\n",
    "            count += 1\n",
    "\n",
    "    # Average the values\n",
    "    if count > 0:\n",
    "        score /= count\n",
    "    else:\n",
    "        score = 0\n",
    "\n",
    "    return score \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"market\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"market\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_similarity(text1,text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_similarity( text1, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'market'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('market', 'NN')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "car = [tag_to_syn(*tag_words) for tag_words in pos_tag(word_tokenize(text1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('market.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_new1 = [s for s in car if s is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "car2 =[tag_to_syn(*tag_words) for tag_words in pos_tag(word_tokenize(text2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('market.n.01')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_new1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('market.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(car2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset_new2 = [m for m in car2 if m is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('market.n.01')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in synset_new1:\n",
    "        #get the similarity value of the most similar word in other sentence\n",
    "    score_best = filter(None, [synset.path_similarity(s) for s in synset_new2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<filter at 0x232e2fa0b00>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-76ea058b11c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mfocus_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfocus_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfocus_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfocus_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-76ea058b11c8>\u001b[0m in \u001b[0;36msentence_similarity\u001b[1;34m(sentence1, sentence2)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynsets1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Get the similarity value of the most similar word in the other sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mbest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msynset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynsets2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# Check that the similarity could have been computed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score = max([synset.path_similarity(ss) for ss in synsets2])\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    " \n",
    "    # Average the values\n",
    "    score /= count\n",
    "    return score\n",
    " \n",
    "sentences = [\n",
    "    \"Dogs are awesome.\",\n",
    "    \"Some gorgeous creatures are felines.\",\n",
    "    \"Dolphins are swimming mammals.\",\n",
    "    \"Cats are beautiful animals.\",\n",
    "]\n",
    " \n",
    "focus_sentence = \"Cats are beautiful animals.\"\n",
    " \n",
    "for sentence in sentences:\n",
    "    print (\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\",  focus_sentence, sentence, sentence_similarity(focus_sentence, sentence))\n",
    "    print (\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\",  sentence, focus_sentence, sentence_similarity(sentence, focus_sentence))\n",
    "    print \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sentence1, sentence2, ignore_integers=False):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = ' '.join([word for word in sentence1.split()])\n",
    "    sentence2 = ' '.join([word for word in sentence2.split()])\n",
    "    tokens1 = word_tokenize(sentence1)\n",
    "    tokens2 = word_tokenize(sentence2)\n",
    "    \n",
    "    # tag\n",
    "    sentence1 = pos_tag(tokens1)\n",
    "    sentence2 = pos_tag(tokens2)\n",
    "\n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    "    print(synsets1)\n",
    "    print(synsets2)\n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    "\n",
    "    score, count = 0.0, 0.0\n",
    "\n",
    "    # For each word in the first sentence\n",
    "    for synset1 in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score=[\n",
    "            wn.wup_similarity(synset1, synset2)\n",
    "            if not isinstance(synset1, str) and not isinstance(synset2, str)\n",
    "            # just in case there are scientific words wordnet does not have\n",
    "            else fuzz.WRatio(str(synset1), str(synset2)) / 100\n",
    "            for synset2 in synsets2\n",
    "        ]\n",
    "        best_score=[s if s else 0 for s in best_score]\n",
    "        # print(synsets1, synsets2)\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score:\n",
    "            score += max(best_score)\n",
    "            count += 1\n",
    "\n",
    "    # Average the values\n",
    "    if count > 0:\n",
    "        score /= count\n",
    "    else:\n",
    "        score = 0\n",
    "\n",
    "    return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet_ic as ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('man.n.01')]\n",
      "[Synset('gentleman.n.01')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_similarity('man', 'gentleman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['terror', 'scourge', 'threat']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('threat.n.04').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'i am good, but i am also bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = text.split(',')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = text.split(',')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am good'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmetizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'Mike went and bought an umbrella for school'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = nltk.word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mike', 'went', 'and', 'bought', 'an', 'umbrella', 'for', 'school']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = nltk.pos_tag(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mike', 'NNP'),\n",
       " ('went', 'VBD'),\n",
       " ('and', 'CC'),\n",
       " ('bought', 'VBD'),\n",
       " ('an', 'DT'),\n",
       " ('umbrella', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('school', 'NN')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\software installations\\anaconda\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (1.16.2)\n",
      "Requirement already satisfied: setuptools in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (40.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (4.43.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\software installations\\anaconda\\lib\\site-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in d:\\software installations\\anaconda\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in d:\\software installations\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\software installations\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\software installations\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software installations\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\software installations\\anaconda\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# It's better to use spacy module to lemmatize the text because we don't have to provide seperate tags\n",
    "\n",
    "!pip install spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('being')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('beryllium.n.01'),\n",
       " Synset('be.v.01'),\n",
       " Synset('be.v.02'),\n",
       " Synset('be.v.03'),\n",
       " Synset('exist.v.01'),\n",
       " Synset('be.v.05'),\n",
       " Synset('equal.v.01'),\n",
       " Synset('constitute.v.01'),\n",
       " Synset('be.v.08'),\n",
       " Synset('embody.v.02'),\n",
       " Synset('be.v.10'),\n",
       " Synset('be.v.11'),\n",
       " Synset('be.v.12'),\n",
       " Synset('cost.v.01')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets([token.lemma_ for token in doc ][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORIGINAL FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize , pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "    \n",
    "#function to convert part of speech tag according to the wordnet context\n",
    "def to_wordnet_tag(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return None\n",
    "\n",
    "\n",
    "    #function to find the synonymous set of the tagged words in the sentences\n",
    "def tag_to_syn(word):\n",
    "    try:\n",
    "        return wn.synsets(word)[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    #function to find the similarity between the sentences using wordnet\n",
    "def sent_similarity(text1,text2):\n",
    "    \n",
    "\n",
    "    #first tokenize the text and tag words with part of speech tagging\n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "    \n",
    "    lem1 = [token.lemma_ for token in doc1]\n",
    "    lem2 = [token.lemma_ for token in doc2]\n",
    "\n",
    "    #Now get the synonymous set of all the tagged words\n",
    "    syns1 = [tag_to_syn(tag_words) for tag_words in lem1]\n",
    "    syns2 = [tag_to_syn(tag_words) for tag_words in lem2]\n",
    "\n",
    "    #We don't want any noise in the synsets so we remove Nones' from the sysnets\n",
    "    synsets1 = [s for s in syns1 if s is not None]\n",
    "    synsets2 = [s for s in syns2 if s is not None]\n",
    "    print(synsets1)\n",
    "    print(synsets2)\n",
    "    \n",
    "    score, count = 0.0,0\n",
    "    \n",
    "    # For each word in the first sentence\n",
    "    for syn1 in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score=[\n",
    "            wn.wup_similarity(syn1, syn2)\n",
    "            if not isinstance(syn1, str) and not isinstance(syn2, str)\n",
    "            # just in case there are scientific words wordnet does not have\n",
    "            else fuzz.WRratio(str(syn1), str(syn2)) / 100\n",
    "            for syn2 in synsets2\n",
    "        ]\n",
    "        best_score=[s if s else 0 for s in best_score]\n",
    "        # print(synsets1, synsets2)\n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score:\n",
    "            score += max(best_score)\n",
    "            count += 1\n",
    "\n",
    "    # Average the values\n",
    "    if count > 0:\n",
    "        score /= count\n",
    "    else:\n",
    "        score = 0\n",
    "\n",
    "    return score \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('giant.n.04')]\n",
      "[Synset('shark.n.01')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5217391304347826"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_similarity('whale','shark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp('life threatning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem1 = [token.lemma_ for token in doc1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life', 'threatne']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
